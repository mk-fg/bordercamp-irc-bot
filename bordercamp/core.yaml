### Default (baseline) configuration parameters.
### DO NOT ever change this config, use -c commandline option instead!


modules:

  _default: # used as a base for all other sections here
    type: relay # for most of them anyway
    # debug: # auto-filled from global "debug" section, if not specified

  # module_name: <-- alias name, used in "route" modules, must not start with an underscore
  #   enabled: true # disabled relays just pass stuff through, routes/channels act as /dev/null
  #   type: <-- one of: relay, channel, route
  #   name: <-- entry_point (python module) name, defaults to alias name above
  #   ... <-- all the contents will be passed as module configuration

  # "relay" modules are imported from python entry_points.
  # "channel" modules are IRC channels (usually start with '#') or nicknames.

  # "route" modules are special:
  #  they poll (or receive msg(s) from) "src" module(s) (name or list)
  #  then pass message through (each) "pipe" module (name or list)
  #   if any "pipe" returns None, message gets discarded
  #   pipes can return a list, meaning that several messages will be relayed further
  #  then pass (a copy of) message(s) to (each) "dst" module(s) (name or list)
  # Routes can have no "dst" and/or "src", in which case they can only be re-used.
  # When route is used as "src", it's "src" and "pipe" parts are
  #  prepended to the corresponding sections of the extension route.
  # When route is used as "pipe", only it's "pipe" is used.
  # When route is used as "dst", only it's "pipe" and "dst" parts are reused.
  # Thus it's possible to create arbitrary filters, static processing pipelines and multiplexing.
  # At least one "route" must be defined for configuration to make sense.

  ## Example configuration:

  # logtail:
  #   monitor: '/var/log/{messages,*.log}' # glob pattern or a list of them

  # pipe_syslog_clean:
  #   name: pipe_sub
  #   src: '\d{4}-\d{2}-\d{2}T(\d{2}:){2}\d{2}\+\d{2}:\d{2} (?P<channel>[\w.]+)<\d+> (?P<msg>.*)'
  #   dst: '\g<channel> \g<msg>'
  # pipe_syslog_resolve:
  #   name: pipe_resolve
  #   addr: '^[\w.]+ .*?\[(\d+|-)\]@(?P<addr>\S+): '

  # bordercamp: # not '#bordercamp' only because it'd have to be quoted every time ;)
  #   type: channel
  #   name: '#bordercamp'

  ## Use "logtail" module to monitor logs for new lines
  ##  then process them with regexes using "pipe_sub" module
  ##  then resolve IPs found there (by regex) with  "pipe_resolve"
  ##  then pass the resulting messages to a "#bordercamp" IRC channel
  # log_report:
  #   type: route
  #   src: logtail
  #   dst: bordercamp
  #   pipe: [pipe_syslog_clean, pipe_syslog_resolve]


relay_defaults:
  # Default settings for relay modules (by entry_point name) and their instances.
  # Can be overidden on per-instance basis.

  _default: # used as a base for all other sections here
    enabled: true

  logtail:
    # Tails logfiles matching specified glob pattern.
    # If directory is matched on startup, all files in it will be monitored.
    monitor: # glob pattern or a list of them, example: '/var/log/{messages,*.log}'
    monitor_exclude: # regex (or list) to match against paths to ignore events for, example: 'debug.*\.log$'
    prepend_filename: false # add source filename to message
    xattr_name: user.bordercamp.logtail.pos # used to mark "last position" in logs b/w restarts
    # processing_delay (float, seconds) allows some time for new log creator
    #  (e.g. syslogd, logrotate) to fill/chmod file and processes all the new lines
    #  in one batch, thus minizing the number of stat()/open() calls.
    processing_delay: 1.0

  feed_syndication:
    # Tails specified rss/atom feeds, converting entries there to irc lines.
    # Uses https://pypi.python.org/pypi/feedparser/ module if "type=feed".
    feeds:
      # Map of "url: parameters" of feed URLs to fetch and processing parameters for each.
      _default: # used as a base for all other feed parameters
        type: feed # "feed" (rss/atom/...) or "reddit-json" (reddit.com/.json and its clones)
        process_max: # max number of (oldest) posts to process/relay on one fetch (0/null/false = inf)
        interval:
          base: 3600 # 1 fetch per hour
          jitter: 0.2 # add +/- 20% jitter to the individual feeds' fetch intervals, so they won't stampede
          fetch_on_startup: true # do fetch right after "jitter" interval after start, not base+jitter as usual
        # Template with individual feed/post parameters to extract.
        # Can be attributes of either "feed" (example: feed.title), "post"
        #  (example: post.summary) or "conf" (config for relay, example: conf.my_comment) object.
        # See one of these docs for the list of usually-available ones:
        #   http://pythonhosted.org/feedparser/common-rss-elements.html
        #   http://pythonhosted.org/feedparser/common-atom-elements.html
        #   http://pythonhosted.org/feedparser/basic.html
        #   http://www.reddit.com/dev/api
        # Examples:
        #   '{feed.title}: {post.title}'
        #   'myfeed: {post.title} ({post.published_parsed})'
        #   '{post.title}:\n{post.summary_detail.value}'
        #   'reddit <{post.url}>: {post.title} ({post.domain})'
        template: '{post.title}'
        deduplication:
          # List of parameters to hash for an unique feed/post-id
          #  as an attributes of either feed/post/conf objects (see "extract" above).
          # Each entry can be a list of keys to fallback to if previous one is missing,
          #  e.g. [feed.id, feed.link, feed.title] will pick guid or link or title,
          #  false/null or empty string can be used to fallback to empty value instead of raising error.
          # Used to skip already-seen posts and detect relevant post modifications.
          # Only one of the fetched posts with the same hash will be relayed.
          - [post.subreddit_id, feed.id, feed.link, feed.title, null]
          - post.id
          # - [feed.id, feed.link, null] # fallback to empty value, if none of these are present
      # 'http://some.site.tld/feed.rss': # for example
      #   interval:
      #     base: 1800

    # deduplication_cache (sqlite db) can be used to avoid
    #   repeating already-relayed posts after bot restart, if set to some persistent path.
    deduplication_cache:
      path: ':memory:' # example: /var/cache/bordercamp/feed_syndication.sqlite
      keep: # cleanup settings for this cache
        cleanup_chance: 0.10 # trigger cleanup on ~1/10 inserts (5% chance)
        per_feed_min: 60 # min latest entries per feed to keep
        timeout_days: 30 # don't remove newer entries than that

    use_cache_headers: true # check/send cache-control, if-modified-since, if-none-match
    request_pool_options: # options for twisted.web.client.HTTPConnectionPool
      maxPersistentPerHost: 10
      cachedConnectionTimeout: 600
      retryAutomatically: true
    ca_certs_files: /etc/ssl/certs/ca-certificates.crt # path or list of paths to tls ca certificates
    user_agent: # to use with all requests, set automatically if empty
    hide_connection_errors: false # lower logging severity of connection errors (for e.g. bad links)

  pipe_sub:
    # Regexp replacements, using python re module.
    # http://docs.python.org/library/re.html#regular-expression-syntax
    # http://docs.python.org/library/re.html#re.sub
    src: # pattern, matched against the message (passed to re.sub())
    dst: # replacement pattern (passed to re.sub())

  pipe_resolve:
    # Resolves part of the message, matched by regexp to/from IP/hostname.
    # Uses canonical name from gethostbyaddr
    #  and first address returned by gethostbyname_ex.
    # Non-matched or unresolvable entries will be passed unchanged.
    addr: # regexp with "addr" or "host" groups, but not both
    short: true # use only first part of the hostnames

  pipe_url_shorten:
    # Shorten matched URLs via some URL shortener API.
    regex: '(?i)\b(?P<url>https?://[^\s<>(),;:]+)' # regexp with "url" group
    length_min: 40 # don't obfuscate stuff that isn't that long to begin with
    api: # defines shortener service or method to use
      # Supported shorteners:
      #  cut - just cuts the url to a given size - int parameter (default: 50)
      #  m29 - http://m29.us/ - no parameters, needs pycrypto module
      type: cut
      parameters:

  pipe_filter:
    # Test patterns in "rules" section for every passed message, stopping on first match.
    # Verdict can be "allow" or "drop", no-match decision is set from "policy".
    # Rules are evaluated in the same order as they appear.
    policy: allow # "allow" or "drop"
    rules:
      # drop_marks: # rule name, only useful for debugging
      #   regex: '--- MARK ---'
      #   action: drop # "drop" and "allow" are simple actions for setting static verdict on match
      # drop_unless_prefix:
      #   regex: '^important:'
      #   # "allow"/"drop" actions have "nomatch" option,
      #   #  which makes them activate only if pattern doesn't match.
      #   action: drop-nomatch
      # msgs_from_self:
      #   regex: '\sbordercamp\[(\d+|-)\]@\w+:\s'
      #   action: limit-1/90 # pass messages only if *rate* is below 1/90 (msg/s)
      # feed_no_questions: # drop rss feed items that have word "question" in the post summary
      #   match: data.post.summary # do event.data.post.summay lookup and filter that via regexp
      #   regex: '(?i)\bquestion\b'
      #   action: drop
      # attr_check:
      #   match: data.post.is_self # no regexp = boolean check if attr exists and non-empty
      #   action: drop

  pipe_exherbo_pkgmon:
    # I use it to monitor #exherbo traffic to pick up patches
    #  and commits for packages which are in my repository (fg_exheres).
    seek:
      # Patterns, used to acquire the package name.
      # Can either have a "name" group directly,
      #  or "patch" with an URL of a diff to download and process.
      - '(^|\s+)!pq\s+(?P<patch>\S+)(\s+::\S+|$)'
      - '^\[[\d:]+\] <irker\d+> .*\bpackages/[\w\-]+/(?P<name>[\w\-]+)/.*( - (?P<patch>https?://\S+)$)?'
    check_path:
      # Glob the given path pattern, emitting a specified line if it exists.
      path: '/var/cache/bordercamp/fg_exheres/packages/*/{name}'
      line: '#exherbo pkg update ({name}): {msg}'

  pipe_snort_nflog:
    # Process Snort IDS log lines, collecting traffic dumps for matched events.
    # Dumps are acquired via zmq from nflog-pcap-recv daemon,
    #   part of nflog-zmq-pcap-pipe thing I use to shuttle traffic to snort:
    #  https://github.com/mk-fg/nflog-zmq-pcap-pipe
    sig_match: ':\s+\[(?P<sig>\d+:\d+:\d+)\]\s' # pattern to match signature ("sig" group)
    traffic_dump:
      # All conditions here are applied on "any" basis.
      path: '/srv/traffic/dump.{ts}' # path to generated dump
      signatures: # list of signatures to generate dumps for, example: [139:1:1, 138:5:1]
      match:
        # List of regexp conditions to test against message,
        #  generating a dump on any match
        # - '\[Classification: Attempted User Privilege Gain\]'
        # - '\[Priority: 1\]'
        # - '\bSENSITIVE-DATA\b'
      match_exclude:
        # If any of these match the message,
        #  dump will not be generated regardless of other conditions
        # - '\bBitTorrent - Torrent File Downloaded\b'
        # - '\burllib\b.* Suspicious User Agent\b'
      nflog_pipe_interface:
        socket: # zmq socket name, example: ipc:///tmp/bif.sock
        timeout: 2.0 # recv/send timeout, to prevent from jamming twisted for too long

  pipe_snort_references:
    # Process Snort IDS log lines, appending available references to them.
    # References are usually URLs to a particular CVE or exploit info,
    #  and are routinely provided along with the rulesets.
    # Two files are used - sid-msg.map with "sid || msg || ref_type,ref || ..." format,
    #  and reference.config, with lines like "config reference: url http://".
    sig_match: ':\s+\[(?P<gid>\d+):(?P<sid>\d+):\d+\]\s' # pattern to match sid and gid (optional)
    gid_ignore: # list of generator ids to skip lookups for, example: [120, 129, 141]
    paths:
      sid_src: /etc/snort/sid-msg.map
      sid_db: /var/cache/bordercamp/sid-msg.db # dbm of id-sources from sid-msg.map
      refs: /etc/snort/reference.config # empty - disable resolving of ref urls
    sid_db_mtime_check_interval: 1800 # interval b/w sid_src/refs vs sid_db mtime checks

  pipe_nflog_snapshots:
    # Test pattern against received msgs, generatng traffic dumps on match.
    patterns: # regexp patterns to match against each received message
      # - '\bnflog-zmq-send\b.*\bWARNING\b'
    traffic_dump:
      path: '/srv/traffic/dump.{ts}' # path to generated dump
      min_interval: 30 # min interval between two dumps, seconds
      nflog_pipe_interface:
        socket: # zmq socket name, example: ipc:///tmp/bif.sock
        timeout: 10.0 # recv/send timeout, to prevent from jamming twisted for too long

  pipe_feed_ostatus_link:
    # Link conversations in status.net (ostatus) feed by appending convo-ids to messages.
    id_length: 3 # length of id (base64 of sha1, so even 3 is quite a lot)
    template:
      note: '/h/{msg.data.post.author_detail.name}: {msg} [{id}]' # for /note type
      comment: '/h/{msg.data.post.author_detail.name}: re:[{id}] - {msg}' # for /comment
      other: '/h/{msg.data.post.author_detail.name}: [misc] {msg}' # for other events

  debug_dumper:
    # Relay that dumps recent messages sent to logging module upon request.
    # Useful only to debug the bot itself.
    # format/datefmt keys are passed to logging.Formatter.
    # Note that handler is attached to root logger, so levels
    #  for "root" and "twisted" loggers should be set accordingly.
    # See http://docs.python.org/library/logging.html
    command: logdump # command to respond to
    direct: true # try to respond directly to sender (if msg is received directly from irc channel)
    level: 0 # int or name, 0 - catch all messages
    capacity: 100 # number of latest messages to keep
    format: '%(asctime)s :: %(levelname)s: %(message)s'
    datefmt: '%Y-%m-%d %H:%M:%S'
    # Signal number or name to dump last
    #  debug lines to a general log (with CRITICAL severity).
    # Can be useful if bot disconnects or becomes unresponsive.
    signal: 'USR1'


core:

  connection: # only one server connection is supported atm
    # twisted endpoint syntax, see twisted.internet.endpoints.html#clientFromString
    endpoint: tcp:host=localhost:port=6667

    nickname: orc
    realname: bordercamp bot
    # username:
    # password:
    # userinfo:

    server:
      # Create a private irc server on the specified endpoint, instead of just connecting there.
      endpoint: # only if enabled, example: tcp:6667:interface=localhost
      name: bordercamp # server name, mostly irrelevant
      motd: # motd to send to all clients, can be multiline or empty
      passwd: # {user: password, ...} dict for ACLs
        # no need to create acl for a bot itself here
        # alice: secretpassword

    reconnect: # see twisted.internet.protocol.ReconnectingClientFactory
      maxDelay: 30
      initialDelay: 1
      factor: 2
      jitter: 0.2
      # maxRetries:

    heartbeat: 60 # interval b/w pings to own hostname

  encoding: utf-8
  nickname_lstrip: '*?@~&%+' # various owner/mode prefixes
  channel_prefix: '#~&' # used to distinguish channel and nickname

  # Emulate xattr module interface with simple k-v db for modules that use it
  # Can be useful if xattr manipulation is limited by permissions (e.g. read-only) or fs capabilities
  # Should be either empty/None/False or a path to shelve db (example - "xattr_emulation: /tmp/xattr.db")
  xattr_emulation: false


debug: # values here can be overidden by special CLI flags
  dry_run: false

logging: # see http://docs.python.org/library/logging.config.html
  # "custom" level means WARNING/DEBUG/NOISE, depending on CLI options
  warnings: true # capture python warnings
  version: 1
  formatters:
    basic:
      format: '%(asctime)s :: %(levelname)s: %(message)s'
      datefmt: '%Y-%m-%d %H:%M:%S'
  handlers:
    console:
      class: logging.StreamHandler
      stream: ext://sys.stdout
      formatter: basic
      level: custom
    # file:
    #   class: logging.handlers.WatchedFileHandler
    #   filename: /var/log/harvestd.log
    #   formatter: basic
    #   encoding: utf-8
    #   level: DEBUG
  loggers:
    twisted:
      handlers: [console]
      level: 0
  root:
    level: 0
